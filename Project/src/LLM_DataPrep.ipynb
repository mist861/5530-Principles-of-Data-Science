{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc0d575-e0ba-4403-a7a7-82b42d168be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f96a6de06424652befa37ca349478f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/502410 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['inputs', 'outputs'],\n",
      "    num_rows: 502410\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load datasets\n",
    "gsm8k_data = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\").to_pandas()\n",
    "dolly_data = load_dataset(\"philschmid/dolly-15k-oai-style\", split=\"train\").to_pandas()\n",
    "pubmed_data = load_dataset(\"fedml/PubMedQA_instruction\", split=\"train\").to_pandas()\n",
    "chatdoctor_data = load_dataset(\"LinhDuong/chatdoctor-200k\", split=\"train\").to_pandas()\n",
    "\n",
    "# Preprocess datasets\n",
    "def preprocess_gsm8k(df):\n",
    "    df['inputs'] = \"Question: \" + df['question']\n",
    "    df['outputs'] = \"Answer: \" + df['answer']\n",
    "    return df[['inputs', 'outputs']]\n",
    "\n",
    "def preprocess_dolly(df):\n",
    "    user_content = df[df['messages'].apply(lambda x: x[0]['role'] == 'user')]['messages'].apply(lambda x: x[0]['content'])\n",
    "    assistant_content = df[df['messages'].apply(lambda x: x[1]['role'] == 'assistant')]['messages'].apply(lambda x: x[1]['content'])\n",
    "    df['inputs'] = user_content\n",
    "    df['outputs'] = assistant_content\n",
    "    return df[['inputs', 'outputs']]\n",
    "\n",
    "def preprocess_pubmed(df):\n",
    "    df['inputs'] = df['instruction'] + \" \" + df['context']\n",
    "    df['outputs'] = df['response']\n",
    "    return df[['inputs', 'outputs']]\n",
    "\n",
    "def preprocess_chatdoctor(df):\n",
    "    df['inputs'] = df['instruction'] + \" \" + df['input']\n",
    "    df['outputs'] = df['output']\n",
    "    return df[['inputs', 'outputs']]\n",
    "\n",
    "gsm8k_preprocessed = preprocess_gsm8k(gsm8k_data)\n",
    "dolly_preprocessed = preprocess_dolly(dolly_data)\n",
    "pubmed_preprocessed = preprocess_pubmed(pubmed_data)\n",
    "chatdoctor_preprocessed = preprocess_chatdoctor(chatdoctor_data)\n",
    "\n",
    "# Concatenate preprocessed datasets\n",
    "combined_df = pd.concat([gsm8k_preprocessed, dolly_preprocessed, pubmed_preprocessed, chatdoctor_preprocessed], ignore_index=True)\n",
    "\n",
    "# Convert back to a Datasets object\n",
    "combined_dataset = Dataset.from_pandas(combined_df)\n",
    "\n",
    "# Save the concatenated dataset\n",
    "combined_dataset.save_to_disk(\"C:/AI_Stuff/data_preprocessed\")\n",
    "\n",
    "# Verify the concatenated dataset\n",
    "print(combined_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c25bba6-47c3-42fb-8960-feac7ce82009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': 'Could you plan a canoe camping trip in Michigan? I want to canoe the river from start to end, and need specific camping locations for each night.',\n",
       " 'outputs': 'I would recommend a canoe camping trip on the Au Sable River in Michigan. The river is about 114 miles long beginning in Grayling and ending in Oscoda. The river features numerous campgrounds which you will be able to camp at each night. \\n\\nDay 1\\nWhitepine Campground\\n\\nDay 2\\nParmalee Campground\\n\\nDay 3\\nMio Campground\\n\\nDay 4\\nAlcona Dam Campground\\n\\nDay 5\\nLoud Dam Campground\\n\\nDay 6\\nEnd at Lake Huron'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset[10726]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "619010e1-05d0-413a-813e-65ea0f826742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d14d72ea724e3c9073ebafdff014fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/502410 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c6881444114bc7858fac9350fae5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/15 shards):   0%|          | 0/502410 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_and_save_dataset(preprocessed_path, processed_path, model_id=\"microsoft/phi-1_5\", max_length=1024):\n",
    "    # Load the preprocessed dataset\n",
    "    preprocessed_dataset = load_from_disk(preprocessed_path)\n",
    "    \n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'left'\n",
    "\n",
    "    # Tokenize function\n",
    "    def tokenize_function(examples):\n",
    "        input_encodings = tokenizer(examples['inputs'], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        output_encodings = tokenizer(examples['outputs'], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        return {\n",
    "            'input_ids': input_encodings['input_ids'],\n",
    "            'attention_mask': input_encodings['attention_mask'],\n",
    "            'labels': output_encodings['input_ids']\n",
    "        }\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    tokenized_dataset = preprocessed_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set format to PyTorch tensors\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "    \n",
    "    # Save the tokenized dataset\n",
    "    tokenized_dataset.save_to_disk(processed_path)\n",
    "\n",
    "# Paths\n",
    "preprocessed_path = \"C:/AI_Stuff/data_preprocessed\"\n",
    "processed_path = \"C:/AI_Stuff/data_processed\"\n",
    "\n",
    "# Run the function\n",
    "tokenize_and_save_dataset(preprocessed_path, processed_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ff97c-1688-4e8c-a155-caf8ecf959bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env2train)",
   "language": "python",
   "name": "env2train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
